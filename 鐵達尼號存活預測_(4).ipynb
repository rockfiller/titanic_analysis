{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJEpSrPiM7IJxVt6lfgc/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockfiller/titanic_analysis/blob/main/%E9%90%B5%E9%81%94%E5%B0%BC%E8%99%9F%E5%AD%98%E6%B4%BB%E9%A0%90%E6%B8%AC_(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5FDDKaeZqZF",
        "outputId": "41f13368-8ade-4f63-9d83-970121631673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Best Score: 0.8092 Best Parameters: {'C': 10, 'solver': 'liblinear'}\n",
            "Decision Tree Best Score: 0.8137 Best Parameters: {'max_depth': 10, 'min_samples_split': 10}\n",
            "Random Forest Best Score: 0.8272 Best Parameters: {'max_depth': 10, 'n_estimators': 200}\n",
            "KNN Best Score: 0.7161 Best Parameters: {'n_neighbors': 9, 'weights': 'distance'}\n",
            "SVM Best Score: 0.8036 Best Parameters: {'C': 100, 'kernel': 'linear'}\n",
            "PyTorch Best Score: 0.8249\n",
            "XGBoost Best Score: 0.8362 Best Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import xgboost as xgb\n",
        "\n",
        "def preprocess_dataset(df):\n",
        "    '''\n",
        "    Preprocess the Titanic dataset by handling missing values and removing irrelevant columns.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Original dataset.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Processed dataset with cleaned and imputed values.\n",
        "    '''\n",
        "    # Remove irrelevant and categorical columns\n",
        "    df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
        "\n",
        "    # Fill missing values in the 'Age' column with the mean age of each Pclass group\n",
        "    df['Age'] = df['Age'].fillna(df.groupby('Pclass')['Age'].transform('mean'))\n",
        "    # Fill missing values in the 'Age' column with the mean age\n",
        "    # df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
        "\n",
        "    # Fill missing values in the 'Embarked' column with the most frequent value (mode)\n",
        "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
        "\n",
        "    # One-Hot Encoding for the 'Sex' and 'Embarked' columns\n",
        "    df = pd.get_dummies(df, columns=['Sex', 'Embarked'], dtype=int)\n",
        "\n",
        "    # Create Fsize as a new numerical feature\n",
        "    df['Fsize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "    # Create Young as a new numerical feature\n",
        "    df['Kid'] = (df['Age'] < 12).astype(int)\n",
        "    return df\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/dsindy/kaggle-titanic/master/data/train.csv')\n",
        "df_train = preprocess_dataset(df)\n",
        "\n",
        "columns_X = df_train.drop(columns=['Survived']).columns\n",
        "columns_y = ['Survived']\n",
        "\n",
        "train_X = df_train[columns_X]\n",
        "train_y = df_train[columns_y]\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=0, max_iter=3000),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=3),\n",
        "    'Random Forest': RandomForestClassifier(max_depth=3, n_estimators=100),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'SVM': SVC(random_state=0)}\n",
        "\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'solver': ['lbfgs', 'liblinear', 'saga']\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'max_depth': [None, 10, 20, 30]\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [10, 50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30]\n",
        "    },\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance']\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "    }}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    try:\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5)\n",
        "        grid_search.fit(train_X, train_y.values.ravel())\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_score = grid_search.best_score_\n",
        "        best_params = grid_search.best_params_\n",
        "        print(f\"{model_name} Best Score: {best_score:.4f} Best Parameters: {best_params}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred for {model_name}: {e}\")\n",
        "\n",
        "# PyTorch Model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(train_X.shape[1], 100)\n",
        "        self.fc2 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "X_tensor = torch.tensor(train_X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(train_y.values, dtype=torch.float32)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "pytorch_model = SimpleNN()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = pytorch_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "pytorch_model.eval()\n",
        "with torch.no_grad():\n",
        "    pytorch_best_score = ((pytorch_model(X_tensor).numpy().flatten() > 0.5) == train_y.values.flatten()).mean()\n",
        "print(f\"PyTorch Best Score: {pytorch_best_score:.4f}\")\n",
        "\n",
        "# XGBoost Model\n",
        "dtrain = xgb.DMatrix(train_X, label=train_y)\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 300]\n",
        "}\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(train_X, train_y.values.ravel())\n",
        "xgb_best_score = grid_search.best_score_\n",
        "xgb_best_params = grid_search.best_params_\n",
        "print(f\"XGBoost Best Score: {xgb_best_score:.4f} Best Parameters: {xgb_best_params}\")\n"
      ]
    }
  ]
}